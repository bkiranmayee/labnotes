04/12/2011

# I am going to start designing some of the graphical figures for the paper/posters. 
# One of my priorities is to use Circos and create an example segment

# I should use a smaller chromosome for the metric or pick a sub-section of one of the larger chromosomes.
# I should also get ready to create the heatmap and dust off the preliminary figures on paired end sequence quality.

# I just reinstalled dependencies for Circos and created a first run. I think that I lost just about all of my prior Circos conf files (no biggie)

# I will try to save a snapshot of my Ubuntu virtualbox just to be safe! That was alot of configuration to do!

# While I wait for the meeting with Steve, how about I do a "proof of concept" heatmap?
	# My goal is to pick a region that will segregate the reads into distinct breeds/types/categories as fluidly as possible
	# The region must have sufficient variability across the dataset, and is variable across as many breeds as possible (one or two breeds can lack CNVs in the region
	# Also, I need to ensure that gaps are not jumping into the region! Since Yali's methods do not take the gaps into account as well as the NGS protocol, I should intersect them out (10kb slop)
	# Set gains at a 1.0 value and losses at a -1.0 value. No significant changes should be represented as a 0. 
	
	# Of course, I can attempt to use the inferred copy number values from the NGS dataset, but that might only apply to the NGS CNVS (the aCGH and SNP datasets do not have such quantification to my knowledge).
	
	
	# I should make a script to pick the locations to automate the process. 
	# Just for the sake of time, I'm going to pick out some sections by eye to check (S:/gliu/Derek/doc_tables/nelore_tables_0310_2011/nelore_included_table4.xlsx) :
		num	chr	start	end	ucsc	#method	#animal	%animal
		840	chr12	68954586	68976034	chr12:68954586-68976034	1	2	2.777777778
		841	chr12	69003586	69026166	chr12:69003586-69026166	1	1	1.388888889
		842	chr12	69071573	69078331	chr12:69071573-69078331	2	2	2.777777778
		843	chr12	69079344	69306090	chr12:69079344-69306090	4	19	26.38888889
		844	chr12	69368080	69534253	chr12:69368080-69534253	3	23	31.94444444
		845	chr12	69701226	69750421	chr12:69701226-69750421	1	1	1.388888889
		846	chr12	69762599	69766199	chr12:69762599-69766199	1	1	1.388888889
		847	chr12	69775204	70161772	chr12:69775204-70161772	4	38	52.77777778
		4930	chr7	39371319	39386034	chr7:39371319-39386034	1	1	1.388888889
		4931	chr7	39791876	39821316	chr7:39791876-39821316	2	16	22.22222222
		4932	chr7	40305198	40320811	chr7:40305198-40320811	1	2	2.777777778
		4933	chr7	40346164	40367982	chr7:40346164-40367982	2	7	9.722222222
		4934	chr7	40641541	40847301	chr7:40641541-40847301	3	13	18.05555556
		4935	chr7	40929532	41395775	chr7:40929532-41395775	1	6	8.333333333
		4936	chr7	41495020	41506133	chr7:41495020-41506133	1	1	1.388888889
		4937	chr7	41524048	41534999	chr7:41524048-41534999	2	13	18.05555556
		4938	chr7	41565035	41570146	chr7:41565035-41570146	1	2	2.777777778
		4939	chr7	41649370	41721610	chr7:41649370-41721610	2	2	2.777777778
		4940	chr7	42032065	42067697	chr7:42032065-42067697	1	3	4.166666667
		4941	chr7	42116393	42139257	chr7:42116393-42139257	1	1	1.388888889
		
	# Sudemant used a 2MB region and estimated copy numbers for each interval. I don't have the luxury of copy number quantitation.
	
	# I will Extract animal information from my table 2 design (merged across methods) in order to get the best picture.
	# I definitely need a script to do this!
	# I wrote a script, and tested it; It might have worked!
	$ perl extract_regions_for_heatmap.pl -c chr12 -s 68500000 -e 70500000 -o chr12_test -i merge_method_sep_animal.tab
	
	# Yes! It did! So I will run cluster 3.0 on this tomorrow. My only concern will be the number of "arrays" (x axis) that will be used in this instance.
	
	# I can actually use Cluster 3.0 in my perl script to streamline things further!
	# Incorporated a subroutine to do this in the extract_regions_for_heatmap.pl script
	
	$ perl extract_regions_for_heatmap.pl -c chr12 -s 68500000 -e 70500000 -o chr12_cluster -i merge_method_sep_animal.tab
	# OK, I don't think that it did the clustering for the file, so I will try to use the command line cluster3.0 program to redo it.
	$ cluster -f chr12_cluster.tab -g 2 -m m -u chr12_command_line
	# OK, that actually worked!

	# I think that the output is marred by the conflicting datasets. Namely, that the aCGH and SNP datasets gum up the works and provide little quantitation.
	# I will try to make a heatmap using the .CN files generated by Alkan's pipeline. This should be an easy script to generate (does not need to intersect files).
	# I should keep this script in the meantime; could be useful for combined dataset analysis.
	
	# Here is the CN file script:
	$ perl extract_cn_for_heatmap.pl -c chr12 -s 68500000 -e 70500000 -o chr12_cn_test
	# It works! But I'll have to design some sort of normalization step or pick a new area!

	# One idea: try the combined breed doc .CN files
	$ perl extract_cn_for_heatmap.pl -c chr12 -s 68500000 -e 70500000 -o chr12_cn_test
	# Got lots of "compatibility error" messages. This makes sense: I ran this on an older version of the interval files
	# Let's redo the alkan pipeline for each of these.
		$ cp ./*/mrsfast/*hits.bed /mnt/data110/dbickhart/cow4_doc/
		$ cat ./limousin/mrsfast/sep_limousin_animals/BTLM*_rem.bed > /mnt/data110/dbickhart/cow4_doc/limousin_hits.bed
		$ cat ./brahman/mrsfast/sep_brahman_animals/BIBR*_rem.bed > /mnt/data110/dbickhart/cow4_doc/brahman_hits.bed
		$ cat ./gir/mrsfast/sep_gir_animals/BIGI*_rem.bed > /mnt/data110/dbickhart/cow4_doc/gir_hits.bed
		$ cat ./jersey/mrsfast/sep_jersey_animals/BTJE*_rem.bed > /mnt/data110/dbickhart/cow4_doc/jersey_hits.bed
		$ cat ./nelore/mrsfast/sep_nelore_animals/BINE*_rem.bed > /mnt/data110/dbickhart/cow4_doc/nelore_hits.bed
		$ cat ./romagnola/mrsfast/sep_romagnola_animals/BTRO*_rem.bed > /mnt/data110/dbickhart/cow4_doc/romagnola_hits.bed
		
		$ for i in *.bed; do perl auto_full_alkan_pipeline.pl --in $i; done
		
	# there's a bit of a disconnect here; I meant to use the above CNVR files in the panther analysis and such, but I did not do this
	# I will have to redo some of those steps and feed the results back into DAVID
		$ for i in *wssd; do prefix=`echo $i | cut -d'_' -f1`; echo $prefix; perl -e 'open (IN, "< $ARGV[0]"); while(<IN>){chomp; print "$_\t$ARGV[1]\n";}' $i $prefix > $i.named; done
		$ for i in *tab; do prefix=`echo $i | cut -d'_' -f1`; echo $prefix; perl -e 'open (IN, "< $ARGV[0]"); while(<IN>){chomp; print "$_\t$ARGV[1]\n";}' $i $prefix > $i.named; done
		$ cat *wssd.named | /mnt/gliu1_usb/dbickhart/BEDTools-Version-2.10.0/bin/mergeBed -i stdin -nms > combined_named_gain.bed
		$ cat *tab.named | /mnt/gliu1_usb/dbickhart/BEDTools-Version-2.10.0/bin/mergeBed -i stdin -nms > combined_named_loss.bed
		
		$ perl -lane '@a = split(/\;/, $F[3]); if (scalar(@a) > 7){ print "$F[0]\t$F[1]\t$F[2]";}' < combined_named_gain.bed > shared_gain.bed
		$ perl -lane '@a = split(/\;/, $F[3]); if (scalar(@a) > 7){ print "$F[0]\t$F[1]\t$F[2]";}' < combined_named_loss.bed > shared_loss.bed
		
		$ for i in *.wssd; do prefix=`echo $i| cut -d'_' -f1`; /mnt/gliu1_usb/dbickhart/BEDTools-Version-2.10.0/bin/subtractBed -a $i -b shared_gain.bed > $prefix.sub.bed; done
		$ for i in *.tab; do prefix=`echo $i| cut -d'_' -f1`; /mnt/gliu1_usb/dbickhart/BEDTools-Version-2.10.0/bin/subtractBed -a $i -b shared_loss.bed > $prefix.sub.loss.bed; done
		
		# Transferred files over to server 2
		$ for i in ./david/*.bed; do perl extract_exons_from_cnvr.pl -i $i -o $i.gids -t cow4_ensGene_2 -d cow4_cnv_gene -f gene_id -c chr -s begin -e end; done
		$ for i in *.gids; do perl -lane 'print $F[1];' < $i > $i.short; done
		
		# Transferred back
		# Run through DAVID
		
		# I think that my cutoff for the "loss events" is too lenient; I will have to increase it to 3 in order to make any headway.
		# I will run that now and make do with the "gain" events in the meantime.
		$ for i in *.bed; do perl auto_full_alkan_pipeline.pl --in $i; done (/mnt/data110/dbickhart/cow4_doc/)
		
	# Now, back to my heatmap script using .CN files
	# Copied all of the redone full breed alkan pipeline files to my local shared folder
	# Targetting the bovine growth hormone gene this time (it was a common theme in the David analysis)
	# Coordinates: chr19 49,440,189  50,086,973
	$ perl extract_cn_for_heatmap.pl -c chr19 -s 49440189 -e 50086973 -o somatotropin_test
	
	# I just want to test this out on the individual animal breed CN files to see if I get good results...
	$ perl extract_cn_for_heatmap.pl -c chr19 -s 49440189 -e 50086973 -o somatotropin_test
	# The input into the perl script's cluster algorithm failed; had to use the command line here:
	$ cluster -f somatotropin_test.tab -g 2 -m m -u somatotropin_test
	# Blackstar got messed up, removing it and rerunning the script
	
	
	# The individual dataset heatmap for the Somatotropin gene is excellent! Clear difference among the Bos indicus and Bos taurus breeds!
	# I need to make a better approximation of ensemblegene locations though, downloaded the ensGene.txt file from UCSC
	$ perl -lane 'if ($F[2] =~ /chrU/){next;}else{print "$F[2]\t$F[4]\t$F[5]";}' < ensGene.txt > cow4_ens_gene.bed
	
	# Designed a very fast and crude perl script to generate coordinates for gnuplotting
	$ perl intersect_ens_gene_windows.pl -c chr19 -s 49440189 -e 50086973 -o somato_ens.dat
	$ gnuplot
		> plot "./somato_ens.dat"
	

________________________________
Circos Diagram
________________________________

# So, the major focal point of my poster will be a large circos diagram with positions of all of the CNVs. 
# I will make up two tracks per animal (gain and loss tracks) and I'll make it relatively simple. 
# I will also use the combined animal data that I used above to create the heatmap
	$ for i in *.wssd; do prefix=`echo $i| cut -d'_' -f1`; echo $prefix; perl -lane '($num) = $F[0] =~ m/chr(\d+)/; print "bt$num\t$F[1]\t$F[2]\t2";' < $i > $prefix.combined_gain.bed; done
	$ for i in *.tab; do prefix=`echo $i| cut -d'_' -f1`; echo $prefix; perl -lane '($num) = $F[0] =~ m/chr(\d+)/; print "bt$num\t$F[1]\t$F[2]\t2";' < $i > $prefix.combined_loss.bed; done
	
	
________________________________
Meeting with Steve
________________________________

# I cannot create my finalized figures/tables until I meet with Steve and determine if I have all the requisite files (Meeting: Thursday the 14th)
# In the meantime, I need to determine how many blackstar reads I have!

# First, I need to convert the old blackstar reads into normal fastq format using a modified version of my convert_seq_fastq.pl script.
	$ for i in *ce.txt.gz; do perl convert_seq_fastq.pl $i; done
	
# No reason why I can't run this on the older blackstar reads in the meantime...
	$ for i in *ence.txt.gz; do perl -e '$f = $ARGV[0]; chomp $f; open (IN, "gunzip -c $f |"); $t = 0; while(<IN>){$t++;} print "$f\t$t\n";' $i >> old_black_num_reads.txt; done
	# now the newer reads.
	$ for i in *fq.gz; do perl -e '$f = $ARGV[0]; chomp $f; open (IN, "gunzip -c $f |"); $t = 0; while(<IN>){$t++;} $g = $t / 4; print "$f\t$g\n";' $i >> new_black_num_reads.txt; done
	
	# Now to get the number of bases per read:
	$ for i in *fq.gz; do perl -e '$f = $ARGV[0]; chomp $f; open (IN, "gunzip -c $f |"); $t = 0; while(<IN>){$b = <IN>; chomp $b; $t = length($b); last;} close IN; print "$f\t$t\n";' $i >> new_black_num_base.txt; done
	$ for i in *fq; do perl -e '$f = $ARGV[0]; chomp $f; open (IN, "< $f"); $t = 0; while(<IN>){$b = <IN>; chomp $b; $t = length($b); last;} close IN; print "$f\t$t\n";' $i >> old_black_num_base.txt; done
	
# It looks like, apart from some older blackstar, new blackstar hiseq and newer nelore data, I have just about all the sequence files! 


_________________________________
Panther analysis
_________________________________

# Yali was kind enough to give me note files on the Panther analysis. I have copied them to S:/gliu/Derek/. 
# Here is the pipeline to generate the gene fasta files that I need to run panther with:
	# On Server 2
	# Upload a bed file with the CNV's (separate gain/loss recommended)
	# Run the following program from George's bin:
	$ extract_exons_from_cnvr.pl -i <my cnvs> -o <the output> -t cow4_ensGene_2 -d cow4_cnv_gene -f pep_id -c chr -s begin -e end
	
	# Run this one-liner on that output:
	$ perl -lane 'print $F[1];' < [previous output] > [list of pep_ids]
	
	# Next, format the files with these one liners:
	$ perl -pi -e 's/\n/ /g' [list of pep_ids]
	$ perl -pi -e 's/^/cat /,s/$/\>fasta\.fa/' [list of pep_ids]
	
	# Now go here and execute the command ids
	$ cd /home/gliu/data10/cattle_gene/assembly4.0/ensemble/bos_taurus/pep/fasta/yaho61
	$ chmod +x [list of pep_ids]
	$ ./[list of pep_ids]
	
	# Now we should have a fasta that can be run in panther:
	perl ./pantherScore.pl -l PANTHER7.0 -D B -V -i /home/gliu/data10/cattle_gene/assembly4.0/ensemble/bos_taurus/pep/fasta/yaho61/[fasta] -o [output file] -n
	
# Now to organize my files and get ready to run panther
	$ cat BIBR*.wssd | ../../../BEDTools-Version-2.10.0/bin/mergeBed -i stdin > brahman_gain.bed
	$ cat BTJE*.wssd | ../../../BEDTools-Version-2.10.0/bin/mergeBed -i stdin > jersey_gain.bed
	$ cat BTHO*.wssd | ../../../BEDTools-Version-2.10.0/bin/mergeBed -i stdin > holstein_gain.bed
	$ cat BTAN*.wssd | ../../../BEDTools-Version-2.10.0/bin/mergeBed -i stdin > angus_gain.bed
	$ cat BIGI*.wssd | ../../../BEDTools-Version-2.10.0/bin/mergeBed -i stdin > gir_gain.bed
	$ cat BTLM*.wssd | ../../../BEDTools-Version-2.10.0/bin/mergeBed -i stdin > limousin_gain.bed
	$ cat BTRO*.wssd | ../../../BEDTools-Version-2.10.0/bin/mergeBed -i stdin > romagnola_gain.bed
	
	# Copied these over to server 2
	# Now, running this on server 2:
	$ cd /mnt/data6/gliu/dbickhart/Panther/
	
	# Testing this on Angus
	$ perl extract_exons_from_cnvr.pl -i angus_gain.bed -o angus_cnv_gain.ids -t cow4_ensGene_2 -d cow4_cnv_gene -f pep_id -c chr -s begin -e end
	$ perl -lane 'print $F[1];' < angus_cnv_gain.ids > angus_cnv_gain.list
	$ perl -i -ne 'print "/mnt/data10/gliu/cattle_gene/assembly4.0/ensemble/bos_taurus/pep/fasta/yaho61/$_";' angus_cnv_gain.list
	$ perl -pi -e 's/\n/ /g' angus_cnv_gain.list
	$ perl -pi -e 's/^/cat /,s/$/\>fasta\.fa/' angus_cnv_gain.list
	# Gave an error that the argument list was too long. I will have to do this using a perl script.
	
	# created a shell script to do the same stuff as above
	$ sh auto_panther.sh angus_gain.bed
	
	$ /home/gliu/data10/pantherScore1.02/pantherScore.pl -l PANTHER7.0 -D B -V -i angus_gain.bed.fa -o angus_gain.bed.panth -n
	# gave me a permission denied!! Damn!
	
	# Going to prepare all of the fastas and then contact yali for george's login just to use the program
	$ for i in *.bed; do sh auto_panther.sh $i; done
	
	# I am going to use George's login in order to run these files
	# running: 
	$ for i in *.bed.fa; do perl /mnt/data10/gliu/pantherScore1.02/pantherScore.pl -o /mnt/data6/gliu/$i.panth -l /mnt/data10/gliu/pantherScore1.02/PANTHER7.0 -D B -V -i $i -n; done
	
	
# OK, so panther is generating data, lets try out DAVID
	# I need to generate gene_id notes for each animal
	# on server 2:
	$ for i in *_gain.bed; do perl extract_exons_from_cnvr.pl -i $i -o $i.gids -t cow4_ensGene_2 -d cow4_cnv_gene -f gene_id -c chr -s begin -e end; done
	
	# Locally:
	$ for i in *.gids; do perl -lane 'print $F[1];' < $i > $i.short
	
	# Now, I want to make a "background file" in order to subtract the common genes from all datasets
	# Actually, scratch that, lets remove the common genes from the lists ahead of time
	# In order to do this, I need to create named bed files and merge them, only printing out the ones that have ALL of the animal names in the merged CNV file
	$ for i in *_gain.bed; do prefix=`echo $i | cut -d'_' -f1`; echo $prefix; perl -e 'open (IN, "< $ARGV[0]"); while(<IN>){chomp; print "$_\t$ARGV[1]\n";}' $i $prefix > $i.named; done
	$ cat *.named | ../BEDTools-Version-2.10.1/bin/mergeBed -i stdin -nms > combined_names.bed
	
	$ perl -lane '@a = split(/\;/, $F[3]); if (scalar(@a) > 6){ print "$F[0]\t$F[1]\t$F[2]";}' < combined_names.bed > combined_shared_cnvs.bed
		$ wc combined_shared_cnvs.bed 
		 281  843 6515 combined_shared_cnvs.bed
		$ wc combined_names.bed 
		  2789  11156 112227 combined_names.bed
	$ for i in *_gain.bed; do ../BEDTools-Version-2.10.1/bin/subtractBed -a $i -b combined_shared_cnvs.bed > $i.sub; done
	
	# On server 2 again:
	$ for i in *.sub; do perl extract_exons_from_cnvr.pl -i $i -o $i.gids -t cow4_ensGene_2 -d cow4_cnv_gene -f gene_id -c chr -s begin -e end; done
	# And locally...
	$ for i in *sub.gids; do perl -lane 'print $F[1];' < $i > $i.short
		> done
	# Excellent!! That worked after I loaded it into DAVID! Good times!
	
	# See the disconnect note above
	
__________________________________
Tables for results
__________________________________

# My big vision is to create two tables for my results section and include a gel picture to show some confirmation is ongoing

# First table will be a brief summary of dataset stats.
# Going to create it using perl one-liners on server 3 and copy-pasting them to excel
	$ for i in *.wssd; do prefix=`echo $i | cut -d'_' -f1`; perl -e '$f = $ARGV[0]; $p = $ARGV[1]; chomp $p; open (IN, "< $f"); $t = 0; $a = 0; while (<IN>){ chomp; @s = split(/\t/); $a += $s[2] - $s[1]; $t++;} print "$p\t$t\t$a\n";' $i $prefix; done
	$ for i in *.tab; do prefix=`echo $i | cut -d'_' -f1`; perl -e '$f = $ARGV[0]; $p = $ARGV[1]; chomp $p; open (IN, "< $f"); $t = 0; $a = 0; while (<IN>){ chomp; @s = split(/\t/); $a += $s[2] - $s[1]; $t++;} print "$p\t$t\t$a\n";' $i $prefix; done
	
	# Now for the paired end data:
	$ for i in *_SV.bed; do perl -e '$t = 0; $a = 0; while(<>){chomp; @s = split(/\t/); $t++; $a += $s[2] - $s[1];} print "$t\t$a\n";' < $i ; done